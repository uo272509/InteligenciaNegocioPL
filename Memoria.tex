\documentclass[
12pt, 
spanish, 
singlespacing,
headsepline
]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,% hyperlinks will be coloured
  linkcolor=blue,% hyperlink text will be blue
}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{float}
\usepackage{underscore}
\usepackage{graphicx}
%Path relative to the .tex file containing the \includegraphics command
\graphicspath{ {./images} }
%\usepackage[export]{adjustbox} % Enable option "frame" for figures
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\newcommand{\image}[2]{
\begin{figure}[H]
	\caption{#1}
	\centering
	\includegraphics[width=1\textwidth]{#2}
\end{figure}
}
\newcommand{\smallimage}[2]{
\begin{figure}[H]
	\caption{#1}
	\centering
	\includegraphics[width=0.5\textwidth]{#2}
\end{figure}
}
\usepackage{xcolor}
\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\parbox{\dimexpr\linewidth-2\fboxsep}{\strut  \texttt{#1}\strut}}}

\author{Marcos Gutiérrez Alonso}
\title{Memoria de las prácticas de Inteligencia de Negocio}
\begin{document}
\begin{titlepage}
	\maketitle
	{\hypersetup{linkcolor=black}
	\tableofcontents
	}
\end{titlepage}

\section{Práctica 1}
\subsection{Sesión 1}
\subsubsection{Scipy 1}
Para añadir más ruido normalizado (pasando de varianza 16 a 64) sólo hay que cambiar el parámetro \texttt{scale}, y para representar las tres imágenes hay que poner en los subplots los valores 131, 132 y 133 (1 fila, 3 columnas y la posición de cada imagen).

El resultado es el siguiente:
\image{Diferentes cantidades de ruido normalizado}{pl1/Figure\_1.png}

\subsubsection{Scipy 2}
En esta parte hay que cambiar la distribución a una uniforme.
La distribución inicial es \texttt{Pareto} y hay que cambiarla a \texttt{Uniform}.

El resultado es el siguiente:
\image{Una distribución uniforme}{pl1/Figure_2.png}

\subsubsection{Scipy 3}
Tras cambiar al test de Wilcoxon: 

El p-valor es: $0.007531$ $\Rightarrow$ Son significativamente diferentes $\Rightarrow$ Se puede deducir que hay uno que es mejor.

\subsubsection{Scipy 4}
\image{Ajustando una distribución de Cauchy a los datos anteriores (curva roja)}{pl1/Figure_4.png}
Estimación kernel (ajuste automático de la distribución), que es una suma ponderada de los puntos en un entorno suave del punto en el que estoy, donde la ponderación depende de la distancia al punto. En otras palabras: Una convolución.

\subsubsection{Scipy 5}
\image{Bola de tamaño 1 para las distancias de Minkowski de órdenes 1, 2, 3 y 4}{pl1/Figure_6-Minkowski.png}

\subsection{Sesión 2: Sklearn}
\subsubsection{Sklearn 1: Univariate}

\image{Comparación de diferentes métodos de ajuste usando una variable}{pl1/Figure_3.png}

Superposición (comparación) de los diferentes métodos:
\image{Comparación de diferentes métodos por separado}{pl1/Figure_4-1.png}

\image{Comparación de diferentes métodos en la misma gráfica}{pl1/Figure_4-2.png}

Parece que Random Forest obtiene los resultados más ajustados.

Probemos con distintos kernels de SVR:

\image{SVR con el kernel poly}{pl1/Figure_5-poly.png}
\image{SVR con el kernel sigmoid}{pl1/Figure_5-sigmoid.png}

Parece que en ningún caso SVR se acerca a los resultados que obtiene Random Forest Regressor. 

\subsubsection{Sklearn 2: Multivariate (ejercicio 7)}
\begin{center}
\emph{Con una sola variable (KBest)}\\
\begin{tabular}{|c|c|c|}
\hline 
Algoritmo & MSE & Score \\ 
\hline 
LIN & 34.5397 & 34.7053 \\ 
\hline 
SVR & 65.9781 & 65.8593 \\ 
\hline 
RNF & 21.5694 & 22.5548 \\ 
\hline 
\end{tabular} 
\linebreak\linebreak\linebreak
\emph{Con todas las variables}\\
\begin{tabular}{|c|c|c|}
\hline 
Algoritmo & MSE & Score \\ 
\hline 
LIN & 34.5397 & 34.7053 \\ 
\hline 
SVR & 65.9781 & 65.8593 \\ 
\hline 
RNF & 21.5032 & 21.7837 \\ 
\hline 
\end{tabular} 
\end{center}

El resultado esperado sería:
\begin{itemize}
\item Con un dato: SVR mejor que LINEAL mejor que RNF
\item Con todos los datos: RNF mejor que LINEAL mejor que SVR	
\end{itemize}

Sin embargo, tanto la regresión lineal como el SVR obtienen el mismo resultado usando una o todas las variables. En ambos casos es el Random Forest el que obtiene los mejores resultados. Cuando usamos una sóla variable obtiene menor error \textbf{y} mayor puntuación, por lo que se deduce que es el mejor \textit{approach}.

\subsection{Sesión 3: Pandas}
Aprendimos a cargar CSVs, cuya única ventaja frente a un excel es que el CSV no tiene límite de filas. En cambio, con el excel sabemos el tipo de cada columna.

Luego manejamos los DataFrames y a hacer estadísticas con sus valores, teniendo en cuenta los valores que son NaN o None. Miramos cómo borrar filas y columnas que contienen valores nulos.

\smallimage{Resumen del dataframe en Pandas, cargado desde Churn\string_Modelling.xlsx}{pl2/Figure_6.png}

Vamos a predecir \textbf{'EstimatedSalary'} (el salario estimado anual) y \textbf{'Exited'}  (si la persona continuó en la empresa o no).

\subsubsection{Predecir el salario estimado}
\paragraph{Eliminando todas las columnas con valores '\textit{NA}'}
\begin{enumerate}
\item Eliminamos todas las filas que contengan un valor '\textit{NA}': \href{https://github.com/uo272509/InteligenciaNegocioPL/blob/c43eff60b8bcd5372c221600c31fed0eea01aec5/Prac1/Parte\%203/pandas-ejercicio-8.py\#L76}{pandas-ejercicio-8.py\#L76}

\item Creamos la variable '\textit{X}' usando '\textit{loc[...]}' para seleccionar las columnas que nos interesan: \href{https://github.com/uo272509/InteligenciaNegocioPL/blob/c43eff60b8bcd5372c221600c31fed0eea01aec5/Prac1/Parte\%203/pandas-ejercicio-8.py\#L35}{pandas-ejercicio-8.py\#L35}

\item Creamos la variable '\textit{Y}' usando '\textit{loc[...]}' para seleccionar las columnas que nos interesan: \href{https://github.com/uo272509/InteligenciaNegocioPL/blob/c43eff60b8bcd5372c221600c31fed0eea01aec5/Prac1/Parte\%203/pandas-ejercicio-8.py\#L38}{pandas-ejercicio-8.py\#L38}

\item Hacemos tres modelos distintos, en este caso: \textit{LinearRegression, SVR (con kernel="poly") y RandomForestRegressor} 

\item Comparamos el error cuadrático medio, el error medio y el \textit{Score} en cada uno de los modelos, usando validación cruzada:
\end{enumerate}
\begin{center}
\emph{Errores de los distintos modelos}\\
\begin{tabular}{|c|c|c|c|}
\hline 
Modelo & MSE & ME & Score \\ 
\hline 
LinearRegression & 3.2998e+09 & \$57444.07 &  3305553891.77\\ 
\hline 
SVR & 3.3024e+09 & \$57466.40 & 3304003674.26\\ 
\hline 
RNF &5.0616e+08 & \$22498.06 & 3587619835.74 \\ 
\hline 
\end{tabular} 
\end{center}
Gracias a estos resultados, podemos deducir que Random Forest obtiene el resultado más óptimo de entre los tres modelos, con un error medio de 22498.06 dólares. Este resultado está muy lejos de ser preciso u óptimo y deja bastante que desear. ¿Tal vez inputando valores haya un mejor resultado?

\paragraph{Inputando datos: Substituyendo valores '\textit{NA}' por 0}
\begin{enumerate}
\item El primer problema que nos encontramos es que en las columnas cuyo tipo de dato es texto ("String"), no podemos inputar los valores perdidos con ceros. Por tanto, lo primero que tendremos que hacer será eliminar las filas en las que cualquiera de dichas columnas: \href{https://github.com/uo272509/InteligenciaNegocioPL/blob/c43eff60b8bcd5372c221600c31fed0eea01aec5/Prac1/Parte\%203/pandas-ejercicio-8.py\#L81}{pandas-ejercicio-8.py\#L81}

\item A continuación, inputamos los valores usando \texttt{\textit{fillna}} con \texttt{\textit{value=0}}: \href{https://github.com/uo272509/InteligenciaNegocioPL/blob/c43eff60b8bcd5372c221600c31fed0eea01aec5/Prac1/Parte\%203/pandas-ejercicio-8.py\#L83}{pandas-ejercicio-8.py\#L83}

\item Repetimos el anterior párrafo con los nuevos datos
\end{enumerate}
\begin{center}
\emph{Errores de los distintos modelos}\\
\begin{tabular}{|c|c|c|c|}
\hline 
Modelo & MSE & ME & Score \\ 
\hline 
LinearRegression & 3.3054e+09 & \$57492.32 &  3310911961.98\\ 
\hline 
SVR & 3.3077e+09 & \$57513.03 & 3309502374.49\\ 
\hline 
RNF & 5.0460e+08 & \$22438.03 & 3590896089.41 \\ 
\hline 
\end{tabular} 
\end{center}

De nuevo es el Random Forest el que obtiene el mejor resultado. Si comparamos el error usando valores inputados (\$22438) con el error usando sólo los valores que no contienen un '\textit{NA}' (\$22498), podemos ver que Random Forest obtiene ligeramente mejores resultados al inputar los datos con ceros.

Es probable que obtuvieramos aún mejores resultados si no inputasemos usando un cero, sino con la media o algún estadístico similar. Esta conclusión la saco a partir del resultado que obtuvieron la Regresión Lineal y el SVR, que con los valores inputados son peores.

\subsubsection{Predecir la permanencia de los trabajadores}
\paragraph{Eliminando todas las columnas con valores '\textit{NA}'}

\begin{center}
\emph{Puntuación de la predicción de la columna 'Exited'}\\
\begin{tabular}{|c|c|}
\hline 
Modelo & Cross-Validation Score (10-fold) \\ 
\hline 
Naive Bayes & 0.2166\\ 
\hline 
Quadratic Discriminant Analysis & 0.1638\\ 
\hline 
5 Nearest Neighbors & 0.2335 \\ 
\hline 
\end{tabular}
\end{center}

\paragraph{Inputando datos: Substituyendo valores '\textit{NA}' por 0}
\begin{center}
\emph{Puntuación de la predicción de la columna 'Exited'}\\
\begin{tabular}{|c|c|}
\hline 
Modelo & Cross-Validation Score (10-fold) \\ 
\hline 
Naive Bayes & 0.2165\\ 
\hline 
Quadratic Discriminant Analysis & 0.1643\\ 
\hline 
5 Nearest Neighbors & 0.2336 \\ 
\hline 
\end{tabular}
\end{center}

Podemos ver que, excepto Naive Bayes, se obtienen resultados ligeramente mejores cuando inputamos valores perdidos.\\
Además, 5NN (5-Nearest Neighbors) obtiene el mejor resultado en ambos casos. De todas formas, esta puntuación tan baja nos indica que en la práctica no se podría predecir valores con ningún tipo de confianza.

\newpage
\section{Práctica 2: Selección de características}
En esta práctica tenemos tres datasets: \textit{iris}, \textit{letter} y \textit{X}, que es un dataset generado con \texttt{\textit{make_classification}}, mientras que los dataset \textit{Iris} y \textit{Letter} se cargan a partir de un csv local. 

Como en esta práctica habrá que repetir un mismo proceso para los tres datasets, intentaré abstraer las funcionalidades a métodos independientes.

\subsection{Eliminación de variables con poca varianza}
En las siguientes dos figuras podemos ver el uso de la función \texttt{\textit{drop_var}} (\href{https://github.com/uo272509/InteligenciaNegocioPL/blob/7f59f8fb2b83986595ab9083fa9a6d28ed39c67c/Prac2/helper.py\#L8}{helper.py\#L8}. 
\smallimage{Llamada a la función de eliminación de variables con poca varianza}{pl2/Figure_7.png}

Esta función:
\begin{itemize}
\item Descarta automáticamente la última columna (presuponiendo que es la de los resultados a predecir).
\item Imprime por consola la varianza de cada columna del dataset mientras almacena la mayor varianza que encuentra.
\item Crea un \texttt{\textit{VarianceThreshold}} del 10\% de la varianza máxima anteriormente obtenida.
\item Hace un \texttt{\textit{fit_transform}} sobre el dataset reducido (sin la columna de los resultados).
\item Imprime por consola la nueva varianza de cada columna.
\item Devuelve el nuevo dataset.
\end{itemize}

\smallimage{Función de eliminación de variables con poca varianza}{pl2/Figure_7.1.png}

\begin{center}
\emph{Varianza de Iris}\\
\begin{tabular}{|c|c|}
\hline 
Columna & Varianza\\ 
\hline 
sepal_length & 0.6857\\ 
\hline
sepal_width & 0.1880\\ 
\hline
petal_length & 3.1132\\ 
\hline
petal_width & 0.5824\\ 
\hline 
\end{tabular}\\

\textit{El \texttt{VarianceThreshold} elimina la columna \textbf{sepal_width}}
\end{center}


\begin{center}
\emph{Varianza de Letter}\\
\begin{tabular}{|c|c|}
\hline 
Columna & Varianza\\ 
\hline 
x-box & 3.6604\\
\hline
y-box & 10.9201\\
\hline
width & 4.0585\\
\hline
high & 5.1139\\
\hline
onpix & 4.7981\\
\hline
x-bar & 4.1048\\
\hline
y-bar & 5.4073\\
\hline
x2bar & 7.2898\\
\hline
y2bar & 5.6683\\
\hline
xybar & 6.1925\\
\hline
x2ybr & 6.9225\\
\hline
xy2br & 4.3290\\
\hline
x-ege & 5.4407\\
\hline
xegvy & 2.3924\\
\hline
y-ege & 6.5899\\
\hline
yegvx & 2.6162\\
\hline 
\end{tabular}\\

\textit{El \texttt{VarianceThreshold} no elimina ninguna columna}
\end{center}


\begin{center}
\emph{Varianza de X}\\
\begin{tabular}{|c|c|}
\hline 
Columna & Varianza\\ 
\hline 
0 & 2.1053\\ 
\hline 
1 & 1.1903\\ 
\hline 
2 & 1.9020\\ 
\hline 
3 & 1.0293\\ 
\hline 
4 & 1.0298\\ 
\hline 
5 & 1.0244\\ 
\hline 
6 & 1.0067\\ 
\hline 
7 & 0.9499\\ 
\hline 
8 & 0.9876\\ 
\hline 
\end{tabular}\\

\textit{El \texttt{VarianceThreshold} no elimina ninguna columna}
\end{center}

\subsection{Eliminación de variables basada en estadísticos univariantes}
Para esta sección hice la función \texttt{\textit{drop_stat_univar}} (\href{https://github.com/uo272509/InteligenciaNegocioPL/blob/7f59f8fb2b83986595ab9083fa9a6d28ed39c67c/Prac2/helper.py\#L32}{helper.py\#L32}. Al igual que antes, descarta la última columna asumiendo que son los resultados a predecir.

Después:
\begin{itemize}
\item Imprime la varianza media del dataset antes de ser procesado.
\item Usa \texttt{\textit{SelectKBest}} con \texttt{\textit{chi²}} para filtrar el dataset e imprime la nueva varianza.
\item Usa \texttt{\textit{SelectPercentile}} con \texttt{\textit{chi²}} y percentil de 0.1 para filtrar el dataset original e imprime la nueva varianza.
\end{itemize}

Así obtenemos la siguiente tabla:

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline 
Dataset & Varianza original & Varianza KBest & Varianza Percentile\\ 
\hline 
Iris & 1.14 & 1.84 & 3.09\\ 
\hline
Letter & 5.34 & 6.02 & 5.44 \\ 
\hline 
\end{tabular}
\end{center}

\subsection{Eliminación recursiva de variables}
En este caso el estimador será \texttt{\textit{SVC}} con kernel \texttt{\textit{linear}}. Este estimador se lo pasamos al \texttt{\textit{RFECV}} (\textit{Recursive feature elimination with cross-validation to select features}), con un "step" y 5-fold (\texttt{\textit{cv=5}}) para la validación cruzada (\href{https://github.com/uo272509/InteligenciaNegocioPL/blob/7f59f8fb2b83986595ab9083fa9a6d28ed39c67c/Prac2/part1.py\#L51}{part1.py\#L51}).

Seguidamente, hacemos un \texttt{\textit{fit}} sobre los datos y obtenemos el resultado.

\subsection{Eliminación de variables usando SelectFromModel}
Este apartado se encuentra en el archivo \href{https://github.com/uo272509/InteligenciaNegocioPL/blob/7f59f8fb2b83986595ab9083fa9a6d28ed39c67c/Prac2/part2.py}{part2.py}.\\

Para esta parte usamos \texttt{\textit{ExtraTreesClassifier}} para calcular la importancia de las distintas variables:
\smallimage{Importancia de las variables según ExtraTreesClassifier}{pl2/Figure_8.png}
\newpage

\section{Práctica 3: Longitud de un cable}
En esta práctica se nos da una tabla cuyas filas representan distintas poblaciones con las siguientes características:
\begin{center}
\begin{tabular}{|c|c|}
\hline 
Columna & Descripción\\ 
\hline \textbf{N} & Número de habitantes\\
\hline \textbf{R} & Radio de la población\\
\hline \textbf{S} y \textbf{T} & \textit{Dos variables desconocidas}\\
\hline \textbf{U} y \textbf{G} & \textit{Dos variables aleatorias}\\
\hline \textbf{L} & Longitud del cable (la solución)\\
\hline 
\end{tabular}
\end{center}

\subsection{Valores perdidos}
El primer paso es eliminar los valores perdidos. Podríamos tratar de inputar los valores, pero por ahora simplemente eliminaremos todas las filas que contengan valores perdidos (\href{https://github.com/uo272509/InteligenciaNegocioPL/blob/2cbbd80f0cc88aa40bd462bec7e322e89a958301/Prac3/main.py\#L69}{main.py\#L69}). Esta decisión nos costará 47 filas de 490 (9.59\% de filas).

\subsection{Normalización de datos}
A continuación tendremos que normalizar los datos. Simplemente usaré el \texttt{\textit{MinMaxScaler().fit_transform(...)}}, que debería de hacer que todos los valores estén entre 0 y 1, agrupando por columnas (es decir, que se normaliza cada columna independientemente de las demás).

Sin embargo, tendremos que guardar el máximo y el mínimo de la columna a predecir, para poder desnormalizar los valores a la hora de calcular el error.

\subsection{Variables relevantes y Regresión}
El siguiente paso consiste en detectar las variables que son irrelevantes. He probado dos métodos para conseguir este fin: 
\begin{itemize}
\item \texttt{\textit{SelectKBest(score_func=f_regression, k=...)}} $\rightarrow$ \href{https://github.com/uo272509/InteligenciaNegocioPL/blob/2cbbd80f0cc88aa40bd462bec7e322e89a958301/Prac3/main.py\#L88}{main.py\#L88}
\item \texttt{\textit{SelectPercentile(score_func=f_regression, percentile=0.1)}} $\rightarrow$ \href{https://github.com/uo272509/InteligenciaNegocioPL/blob/2cbbd80f0cc88aa40bd462bec7e322e89a958301/Prac3/main.py\#L89}{main.py\#L88}
\end{itemize}

Además, he probado \texttt{\textit{SelectKBest}} con \textit{\texttt{k=1, 2, ..., 5}}, obteniendo los siguientes resultados:

\begin{center}
\emph{Columnas que KBest elige, en función de \texttt{\textit{k}}}\\
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline k=... & N & R & S & T & U & G & L\\ 
\hline 1 &  &  & X &  &  &  &  \\
\hline 2 &  &  & X &  & X &  &  \\
\hline 3 &  & X & X &  & X &  &  \\
\hline 4 &  & X & X & X & X &  &  \\
\hline 5 &  & X & X & X & X & X &  \\
\hline 
\end{tabular}
\end{center}

Por tanto, partimos del hecho de que KBest selecciona las columnas que menos correlación tienen con el problema. Serán las que tienen más varianza, pero desde luego no está seleccionando las columnas adecuadas. Aún así:

\image{Error medio con KBest en función del \textit{\texttt{k}}}{pl3/Figure_8_Kbest_MeanError.png}

Con Regresión Lineal y \texttt{\textit{k=1}}podemos llegar a obtener más de un 95\% de aciertos... Lo cual no tiene mucho sentido teniendo en cuenta que sólo tiene como parámetro S, que es una función desconocida.

El error medio final que he obtenido ha sido \textbf{680.6916 metros} (\href{https://github.com/uo272509/InteligenciaNegocioPL/blob/2cbbd80f0cc88aa40bd462bec7e322e89a958301/Prac3/main.py\#L115}{main.py\#L115})
\newpage

\section{Práctica 4: Análisis gráfico de los datos}
\subsection{K-Best y PCA}
\image{K-Best}{pl4/Figure_1.png}
\image{PCA}{pl4/Figure_2.png}
\image{K-Best + PCA}{pl4/Figure_3.png}
\subsection{LFA}
\image{LFA}{pl4/Figure_4.png}
\subsection{MDS}
\image{MDS}{pl4/Figure_5.png}
\subsection{LLE}
\image{LLE}{pl4/Figure_6.png}
\subsection{Opcional 1: PCA + t-SNE}
\image{PCA + t-SNE}{pl4/Figure_7.png}
\end{document}
