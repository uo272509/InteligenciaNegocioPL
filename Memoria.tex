\documentclass[
12pt, 
spanish, 
singlespacing,
headsepline
]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage[hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{float}
\usepackage{underscore}
\usepackage{graphicx}
%Path relative to the .tex file containing the \includegraphics command
\graphicspath{ {./images} }
%\usepackage[export]{adjustbox} % Enable option "frame" for figures
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\newcommand{\image}[2]{
\begin{figure}[H]
	\caption{#1}
	\centering
	\includegraphics[width=1\textwidth]{#2}
\end{figure}
}
\usepackage{xcolor}
\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\parbox{\dimexpr\linewidth-2\fboxsep}{\strut  \texttt{#1}\strut}}}

\author{Marcos Gutiérrez Alonso}
\title{Memoria de las prácticas de Inteligencia de Negocio}
\begin{document}
\begin{titlepage}
	\maketitle
	\tableofcontents
\end{titlepage}

\section{Práctica 1.0: Scipy}
\subsection{Scipy 1}
Para añadir más ruido normalizado (pasando de varianza 16 a 64) sólo hay que cambiar el parámetro \texttt{scale}, y para representar las tres imágenes hay que poner en los subplots los valores 131, 132 y 133 (1 fila, 3 columnas y la posición de cada imagen).

El resultado es el siguiente:
\image{Diferentes cantidades de ruido normalizado}{pl1/Figure\_1.png}

\subsection{Scipy 2}
En esta parte hay que cambiar la distribución a una uniforme.
La distribución inicial es \texttt{Pareto} y hay que cambiarla a \texttt{Uniform}.

El resultado es el siguiente:
\image{Una distribución uniforme}{pl1/Figure_2.png}

\subsection{Scipy 3}
Tras cambiar al test de Wilcoxon: 

El p-valor es: $0.007531$ $\Rightarrow$ Son significativamente diferentes $\Rightarrow$ Hay uno que es mejor

\subsection{Scipy 4}
Estimación kernel (ajuste automático de la distribución), que es una suma ponderada de los puntos en un entorno suave del punto en el que estoy, donde la ponderación depende de la distancia al punto. En otras palabras: Una \textbf{convolución}.

\section{Práctica 1.1: Sklearn}
\subsection{Sklearn 1: Univariate}

\image{Comparación de diferentes métodos de ajuste usando una variable}{pl1/Figure_3.png}

Superposición (comparación) de los diferentes métodos:
\image{Comparación de diferentes métodos por separado}{pl1/Figure_4-1.png}

\image{Comparación de diferentes métodos en la misma gráfica}{pl1/Figure_4-2.png}

Parece que Random Forest obtiene los resultados más ajustados.

Probemos con distintos kernels de SVR:

\image{SVR con el kernel poly}{pl1/Figure_5-poly.png}
\image{SVR con el kernel sigmoid}{pl1/Figure_5-sigmoid.png}

\subsection{Sklearn 2: Multivariate (ejercicio 7)}
\begin{itemize}
\item Con un dato: SVR mejor que LINEAL mejor que RNF
\item Con todos los datos: RNF mejor que LINEAL mejor que SVR	
\end{itemize}

\section{Práctica 1.2: Pandas}
\subsection{Pandas 1, 2, 3 y 4}
Aprendimos a cargar CSVs, cuya única ventaja frente a un excel es que el CSV no tiene límite de filas. En cambio, con el excel sabemos el tipo de cada columna.

Luego manejamos los DataFrames y a hacer estadísticas con sus valores, teniendo en cuenta los valores que son NaN o None. Miramos cómo borrar filas y columnas que contienen valores nulos.

\image{Resumen del dataframe en Pandas, cargado desde Churn\string_Modelling.xlsx}{pl2/Figure_6.png}

\section{Práctica 1.3: Más Pandas}
\subsection{Pandas 5}

\section{Práctica 2: Selección de características}
En esta práctica tenemos tres datasets:
\begin{itemize}
\item iris
\item letters
\item X
\end{itemize}

X lo he convertido a un DataFrame (\code{pd.DataFrame(X)}) para poder manejarlo igual que los otros dos dataset.

Como en esta práctica habrá que repetir un mismo proceso para los tres datasets, intentaré abstraer las funcionalidades a métodos independientes.

\subsection{Eliminación de variables con poca varianza}
\image{Llamada a la función de eliminación de variables con poca varianza}{pl2/Figure_7.png}
\image{Función de eliminación de variables con poca varianza}{pl2/Figure_7.1.png}

\section{Práctica 3: Longitud de un cable}
En esta práctica se nos da una tabla cuyas filas representan distintas poblaciones con las siguientes características:
\begin{itemize}
\item \textbf{N}: "Número de habitantes"
\item \textbf{R}: "Radio de la población"
\item \textit{Dos variables desconocidas}
\item \textit{Dos variables aleatorias}
\item \textbf{L}: "Longitud del cable" (la solución)
\end{itemize}

\subsection{Valores perdidos}
El primer paso es eliminar los valores perdidos. Podríamos tratar de inputar los valores, pero por ahora simplemente eliminaremos todas las filas que contengan valores perdidos con el comando \code{cables.dropna(how='any', inplace=True)}. Esta decisión nos costará 47 filas de 490 (9.59\% de filas).

\subsection{Normalización de datos}
A continuación tendremos que normalizar los datos. Simplemente usaré el \code{MinMaxScaler().fit_transform(...)}, que debería de hacer que todos los valores estén entre 0 y 1, agrupando por columnas (es decir, que se normaliza cada columna independientemente de las demás).

\subsection{Variables relevantes}
El siguiente paso consiste en detectar las variables que son irrelevantes. He probado dos métodos para conseguir este fin: \code{SelectKBest(score_func=f_regression, k=...)} y \code{SelectPercentile(score_func=f_regression, percentile=0.1)}.

\subsection{LinearModel y SelectKBest}

\section{Práctica 4: Análisis gráfico de los datos}
\subsection{K-Best y PCA}
\image{K-Best}{pl4/Figure_1.png}
\image{PCA}{pl4/Figure_2.png}
\image{K-Best + PCA}{pl4/Figure_3.png}
\subsection{LFA}
\image{LFA}{pl4/Figure_4.png}
\subsection{MDS}
\image{MDS}{pl4/Figure_5.png}
\subsection{LLE}
\image{LLE}{pl4/Figure_6.png}
\subsection{Opcional 1: PCA + t-SNE}
\image{PCA + t-SNE}{pl4/Figure_7.png}
\end{document}
